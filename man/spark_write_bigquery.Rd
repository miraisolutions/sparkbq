% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_write_bigquery.R
\name{spark_write_bigquery}
\alias{spark_write_bigquery}
\title{Writing data to Google BigQuery}
\usage{
spark_write_bigquery(data, projectId, datasetId, tableId, gcsBucket,
  datasetLocation, mode = "error", partition_by = NULL)
}
\arguments{
\item{data}{Spark DataFrame to write to Google BigQuery.}

\item{projectId}{Google Cloud Platform project ID for BigQuery.}

\item{datasetId}{Google BigQuery dataset ID (may contain letters, numbers and underscores).}

\item{tableId}{Google BigQuery table ID (may contain letters, numbers and underscores).}

\item{gcsBucket}{Google Cloud Storage bucket used for temporary BigQuery files.}

\item{datasetLocation}{Google BigQuery dataset location ("EU" or "US").
This parameter can be found in the Google BigQuery web UI, under the "Dataset Details".}

\item{mode}{Specifies the behavior when data or table already exist. One of "overwrite",
"append", "ignore" or "error" (default).}

\item{partition_by}{vector of column names used for partitioning}
}
\description{
This function writes data to a Google BigQuery table.
}
\references{
\url{https://cloud.google.com/bigquery/docs/datasets}
\url{https://cloud.google.com/bigquery/docs/tables}
\url{https://cloud.google.com/bigquery/docs/reference/standard-sql/}
}
\seealso{
\code{\link[sparklyr]{spark_write_source}}, \code{\link{spark_read_bigquery}}

Other Spark serialization routines: \code{\link{spark_read_bigquery}}
}
