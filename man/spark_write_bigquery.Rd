% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_write_bigquery.R
\name{spark_write_bigquery}
\alias{spark_write_bigquery}
\title{Writing data to Google BigQuery}
\usage{
spark_write_bigquery(
  data,
  projectId = default_project_id(),
  datasetId,
  tableId,
  serviceAccountKeyFile = default_service_account_key_file(),
  additionalParameters = NULL,
  mode = "error",
  ...
)
}
\arguments{
\item{data}{Spark DataFrame to write to Google BigQuery.}

\item{projectId}{Google Cloud Platform project ID of BigQuery dataset.
Defaults to \code{default_project_id()}.}

\item{datasetId}{Google BigQuery dataset ID (may contain letters, numbers and underscores).}

\item{tableId}{Google BigQuery table ID (may contain letters, numbers and underscores).}

\item{serviceAccountKeyFile}{Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).}

\item{additionalParameters}{\href{https://github.com/GoogleCloudDataproc/spark-bigquery-connector?tab=readme-ov-file#properties}{Additional Spark BigQuery connector options}.}

\item{mode}{Specifies the behavior when data or table already exist. One of "overwrite",
"append", "ignore" or "error" (default).}

\item{...}{Additional arguments passed to \code{\link[sparklyr]{spark_write_source}}.}
}
\value{
\code{NULL}. This is a side-effecting function.
}
\description{
This function writes data to a Google BigQuery table.
}
\details{
Data is written directly to BigQuery using the
\href{https://cloud.google.com/bigquery/docs/write-api}{BigQuery Storage Write API}.
}
\examples{
\dontrun{
config <- spark_config()

sc <- spark_connect(master = "local", config = config)

bigquery_defaults(
  projectId = "<your_project_id>",
  serviceAccountKeyFile = "<your_service_account_key_file>")

# Copy mtcars to Spark
spark_mtcars <- dplyr::copy_to(sc, mtcars, "spark_mtcars", overwrite = TRUE)

spark_write_bigquery(
  data = spark_mtcars,
  datasetId = "<your_dataset_id>",
  tableId = "mtcars",
  mode = "overwrite")
}
}
\references{
\url{https://github.com/GoogleCloudDataproc/spark-bigquery-connector}

\url{https://cloud.google.com/bigquery/docs/datasets}

\url{https://cloud.google.com/bigquery/docs/tables}

\url{https://cloud.google.com/bigquery/docs/reference/standard-sql/}

\url{https://cloud.google.com/bigquery/pricing}

\url{https://cloud.google.com/bigquery/docs/dataset-locations}

\url{https://cloud.google.com/docs/authentication/}

\url{https://cloud.google.com/bigquery/docs/authentication/}

\url{https://cloud.google.com/bigquery/docs/write-api}
}
\seealso{
\code{\link[sparklyr]{spark_write_source}}, \code{\link{spark_read_bigquery}},
\code{\link{bigquery_defaults}}

Other Spark serialization routines: 
\code{\link{spark_read_bigquery}()}
}
\concept{Spark serialization routines}
\keyword{connection}
\keyword{database}
