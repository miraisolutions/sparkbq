% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/defaults.R
\name{bigquery_defaults}
\alias{bigquery_defaults}
\title{Google BigQuery Default Settings}
\usage{
bigquery_defaults(billingProjectId, gcsBucket, datasetLocation, type = NULL)
}
\arguments{
\item{billingProjectId}{Default Google Cloud Platform project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.}

\item{gcsBucket}{Default Google Cloud Storage bucket used for temporary BigQuery files.
This should be the name of an existing storage bucket.}

\item{datasetLocation}{Default Google BigQuery dataset location ("EU" or "US").}

\item{type}{Default BigQuery import/export type to use. Options include "direct",
"parquet", "avro", "orc", "json" and "csv". If not set, it defaults to
\code{NULL}, meaning that the default spark-bigquery import/export mechanism
will be used (i.e. "direct"). Please note that only "direct" and "avro"
are supported for both importing and exporting.
See the table below for supported type and import/export combinations.

\tabular{lcccccc}{
                                         \tab Direct \tab Parquet \tab Avro \tab ORC \tab JSON \tab CSV  \cr
  Import to Spark (export from BigQuery) \tab X      \tab         \tab X    \tab     \tab X    \tab X    \cr
  Export from Spark (import to BigQuery) \tab X      \tab X       \tab X    \tab X   \tab      \tab      \cr
}}
}
\value{
A \code{list} of set options with previous values.
}
\description{
Sets default values for several Google BigQuery related settings.
}
\seealso{
\code{\link{spark_read_bigquery}}, \code{\link{spark_write_bigquery}},
\code{\link{default_billing_project_id}}, \code{\link{default_gcs_bucket}},
\code{\link{default_dataset_location}}
}
\keyword{connection}
\keyword{database,}
