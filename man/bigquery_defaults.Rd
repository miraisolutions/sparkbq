% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/defaults.R
\name{bigquery_defaults}
\alias{bigquery_defaults}
\title{Google BigQuery Default Settings}
\usage{
bigquery_defaults(
  billingProjectId,
  materializationProject = billingProjectId,
  materializationDataset = NULL,
  gcsBucket = NULL,
  serviceAccountKeyFile = NULL,
  writeMethod = "direct"
)
}
\arguments{
\item{billingProjectId}{Default Google Cloud Platform (GCP) project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.}

\item{materializationProject}{Project to use for materializing SQL queries. See also
\code{materializationDataset}. Defaults to billing project (\code{billingProjectId}).}

\item{materializationDataset}{Dataset (in materialization project) which is used for
materializing SQL queries (see argument \code{sqlQuery} in \code{\link{spark_read_bigquery}}).
The GCP user (see \code{serviceAccountKeyFile}) needs to have table creation permission in
that dataset. Note that according to
\url{https://cloud.google.com/bigquery/docs/writing-results#temporary_and_permanent_tables}
the queried tables must be in the same location as the materialization dataset.}

\item{gcsBucket}{Google Cloud Storage (GCS) bucket to use for storing temporary files.
Temporary files are used when importing through BigQuery load jobs and exporting through
BigQuery extraction jobs (i.e. when using data extracts such as Parquet, Avro, ORC, ...).
The service account specified in \code{serviceAccountKeyFile} needs to be given appropriate
rights. This should be the name of an existing storage bucket.}

\item{serviceAccountKeyFile}{Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).
If not specified, Google application default credentials (ADC) will be used, which is the default.}

\item{writeMethod}{Default BigQuery write method ("direct" or "gcs").}
}
\value{
A \code{list} of set options with previous values.
}
\description{
Sets default values for several Google BigQuery related settings.
}
\references{
\url{https://github.com/GoogleCloudDataproc/spark-bigquery-connector}
\url{https://cloud.google.com/bigquery/pricing}
\url{https://cloud.google.com/bigquery/docs/dataset-locations}
\url{https://cloud.google.com/bigquery/docs/authentication/service-account-file}
\url{https://cloud.google.com/docs/authentication/}
\url{https://cloud.google.com/bigquery/docs/authentication/}
\url{https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet}
\url{https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro}
\url{https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc}
\url{https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json}
\url{https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv}
}
\seealso{
\code{\link{spark_read_bigquery}}, \code{\link{spark_write_bigquery}},
\code{\link{default_billing_project_id}}, \code{\link{default_materialization_project}},
\code{\link{default_materialization_dataset}}, \code{\link{default_gcs_bucket}},
\code{\link{default_service_account_key_file}}, \code{\link{default_write_method}}
}
\keyword{connection}
\keyword{database}
